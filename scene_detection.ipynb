{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eQT_rD1sLU9yVx8u0ZHrYRze-yGEAOVQ",
      "authorship_tag": "ABX9TyPW62ZHLdDRtdHSv+YIIktC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuzoo0226/3D-OVS/blob/main/scene_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wErBwASMQYq_",
        "outputId": "ac264ee1-f0bf-40d3-df9f-ddb7062b9cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [80.4 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,002 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,237 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,789 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [43.0 kB]\n",
            "Hit:16 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Fetched 24.4 MB in 3s (7,188 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Colab で一度だけ実行\n",
        "!pip -q install torch torchvision torchaudio librosa soundfile opencv-python\n",
        "!apt -y -q update && apt -y -q install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, math, os, sys\n",
        "import openai, base64\n",
        "import glob, csv, shutil\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "import cv2\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import librosa\n",
        "from google.colab import userdata\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AIp2TKPRDNj",
        "outputId": "28dd9ed7-1b58-4905-8d16-49647f5f294d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 設定（ここだけ変えればOK） ======\n",
        "CFG = {\n",
        "    # 入力ファイル\n",
        "    # \"annotations\": \"\",          # 動画一覧（前のままの形式でOK）\n",
        "    # \"binary_timestamps\": \"/content/positive_times.json\", # ポジティブ点 or 区間ファイル（下に例）\n",
        "    \"dataset_json\": \"/content/drive/MyDrive/ResearchIntern/InteractiveVQA/annotation/action_segments.json\",  # ←あなたの JSON パスに変更\n",
        "\n",
        "    # 出力\n",
        "    \"work_dir\": \"/content/runs/exp_bin1\",\n",
        "\n",
        "    # 時間同期/サンプリング\n",
        "    \"target_fps\": 2.0,       # フレームサンプリングFPS\n",
        "    \"audio_sr\": 16000,       # 音声リサンプリング\n",
        "    \"audio_win\": 0.5,        # 各フレームの±(audio_win/2)で窓切り出し\n",
        "\n",
        "    # 特徴量\n",
        "    \"resize\": 224,\n",
        "    \"center_crop\": 224,\n",
        "    \"n_mels\": 64,\n",
        "    \"n_fft\": 400,            # 25ms @16k\n",
        "    \"hop_length\": 160,       # 10ms @16k\n",
        "\n",
        "    # 学習\n",
        "    \"epochs\": 5,\n",
        "    \"batch_size\": 4,\n",
        "    \"lr\": 1e-3,\n",
        "    \"pos_weight\": None,      # POSクラスの重み（不均衡対策: 例えば 2.0）\n",
        "\n",
        "    # データ分割\n",
        "    \"split_ratio\": 0.8,      # train/val\n",
        "    \"seed\": 42,\n",
        "\n",
        "    # ポジティブ点→区間化の幅（秒）\n",
        "    \"pos_window\": 0.6,\n",
        "}\n",
        "\n",
        "os.makedirs(CFG[\"work_dir\"], exist_ok=True)\n"
      ],
      "metadata": {
        "id": "0FcpAn-6RIgp"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class SatsudoraDataset:\n",
        "    # id: int\n",
        "    talk_start_time: float\n",
        "    talk_end_time: float\n",
        "    action_start_time: float\n",
        "    action_end_time: float\n",
        "    _from: str\n",
        "    _to: str\n",
        "    talk_text: str\n",
        "    objective_nonverbal: str\n",
        "    subjective_nonverbal: str\n",
        "\n",
        "@dataclass\n",
        "class AnnotationInfo:\n",
        "    annotation_path: str\n",
        "    video_path: str\n",
        "    data: List[SatsudoraDataset]"
      ],
      "metadata": {
        "id": "4BSpZt9uSI8y"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadDataset():\n",
        "    def __init__(self):\n",
        "        # 環境変数からAPIキーを取得\n",
        "        self.api_key = userdata.get('OPENAI_API')\n",
        "        self.gpt_version = \"gpt-4o\"\n",
        "\n",
        "        self.annotation_dir = \"/content/drive/MyDrive/ResearchIntern/EscortData/202501_サツドラ/【最終版】データセット/annotations_csv\"\n",
        "        # self.satsudora_dataset = self.load_satsudora_datasets(annotation_dir=annotation_dir)\n",
        "        # print(self.satsudora_dataset)\n",
        "        # self.satsudora_dataset[0].data[0].talk_text\n",
        "\n",
        "        if self.api_key is None:\n",
        "            print(\"OPENAI_API is not set.\")\n",
        "            sys.exit()\n",
        "        else:\n",
        "            self.client = openai.OpenAI(api_key=self.api_key)\n",
        "\n",
        "    def load_satsudora_datasets(self, annotation_dir: str) -> List[List[SatsudoraDataset]]:\n",
        "        datasets: List[AnnotationInfo] = []\n",
        "        csv_files = sorted(glob.glob(os.path.join(annotation_dir, \"*.csv\")))\n",
        "        # video_files = sorted(glob.glob(os.path.join(video_dir, \"*.mp4 のコピー\")))\n",
        "        for idx, csv_file in enumerate(csv_files):\n",
        "            datasets.append(\n",
        "                AnnotationInfo(\n",
        "                    annotation_path=csv_file,\n",
        "                    video_path=csv_file.replace(\"annotations_csv\", \"videos\").replace(\".csv\", \".mp4 のコピー\"),\n",
        "                    data=self._load_csv(csv_file),\n",
        "                )\n",
        "            )\n",
        "        return datasets\n",
        "\n",
        "    @staticmethod\n",
        "    def time_to_seconds(timestr: str) -> float:\n",
        "        \"\"\" 'hh:mm:ss.xxx' 形式を秒数(float)に変換 \"\"\"\n",
        "        if not timestr:\n",
        "            return None\n",
        "        h, m, s = timestr.split(\":\")\n",
        "        return int(h) * 3600 + int(m) * 60 + float(s)\n",
        "\n",
        "    def _load_csv(self, csv_path: str) -> List[SatsudoraDataset]:\n",
        "        datasets: List[SatsudoraDataset] = []\n",
        "        with open(csv_path, newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)  # ヘッダー行を利用して辞書として読み込む\n",
        "            for idx, row in enumerate(reader):\n",
        "                try:\n",
        "                    datasets.append(\n",
        "                        SatsudoraDataset(\n",
        "                            # id=idx,\n",
        "                            talk_start_time=self.time_to_seconds(row[\"発話開始タイムスタンプ\"]),\n",
        "                            talk_end_time=self.time_to_seconds(row[\"発話終了タイムスタンプ\"]),\n",
        "                            _from=row[\"発話者\"],\n",
        "                            _to=row[\"宛先\"],\n",
        "                            talk_text=row[\"発話内容\"],\n",
        "                            action_start_time=self.time_to_seconds(row[\"行動開始タイムスタンプ\"]),\n",
        "                            action_end_time=self.time_to_seconds(row[\"行動終了タイムスタンプ\"]),\n",
        "                            objective_nonverbal=row[\"客観的非言語行動\"],\n",
        "                            subjective_nonverbal=row[\"主観的非言語行動\"],\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                except (KeyError, ValueError) as e:\n",
        "                    print(f\"⚠️ スキップしました: {row} (エラー: {e})\")\n",
        "        return datasets\n",
        "\n",
        "    def set_inital_prompt(self) -> None:\n",
        "        self.messages = [\n",
        "            # inital promptの定義\n",
        "            {\"role\": \"system\", \"content\": \"以下は、実際の店舗内でヒューマノイドロボット（Sota）とお客さんが対話している動画から切り出した複数のフレーム画像です。それぞれのフレームをよく観察し、お客さんの具体的な非言語的行動を詳細に説明してください。特に、お客さんの目に見える行動、ジェスチャー、姿勢、顔の向き、ロボットや周囲の環境との物理的な関わりに注目してください。\"},\n",
        "            {\"role\": \"system\", \"content\": \"抽象的で曖昧な表現は避け、お客さんが何をしているのかを明確に述べてください。（例：「お客さんは棚を指さしている」「お客さんはロボットの方に身を乗り出している」「お客さんは右手で商品を持っている」など）\"},\n",
        "            {\"role\": \"system\", \"content\": \"出力は以下の形式でお願いします：\"},\n",
        "            {\"role\": \"system\", \"content\": \"1. **観察された客観的な行動** – お客さんが実際にしている行動を簡潔に説明する\"},\n",
        "            {\"role\": \"system\", \"content\": \"1. **観察された行動に対する主観的な評価** – その事象に対して考えられること，なぜそのような行動をとっているかなどの理由を簡潔に説明する\"},\n",
        "            {\"role\": \"system\", \"content\": \"2. **根拠** – なぜその行動と解釈したのか（例：手の位置、体の向き、物体との関わりなどに基づく）\"},\n",
        "        ]\n",
        "\n",
        "    def extract_uniform_frames(\n",
        "        self,\n",
        "        video_path: str,\n",
        "        k: int = 5,\n",
        "        out_dir: Optional[str] = None,\n",
        "        prefix: str = \"frame\",\n",
        "        jpg_quality: int = 95,\n",
        "        rewrite: bool = False\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        p = Path(video_path)\n",
        "        out_dir = out_dir or str((p.parent / f\"{p.stem}_uniform_frames\").resolve())\n",
        "\n",
        "        if rewrite:\n",
        "            try:\n",
        "                os.rmdir(out_dir)\n",
        "            except Exception as e:\n",
        "                # check = input(f\"{out_dir}を削除しますがよろしいですか？ [y/N]>>\")\n",
        "                # if check == \"y\":\n",
        "                shutil.rmtree(out_dir)\n",
        "                    # print(e)\n",
        "                # else:\n",
        "                    # pass\n",
        "\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        cap = cv2.VideoCapture(str(p))\n",
        "        if not cap.isOpened():\n",
        "            raise RuntimeError(f\"Failed to open video: {video_path}\")\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = float(cap.get(cv2.CAP_PROP_FPS)) or 30.0\n",
        "        if total_frames <= 0:\n",
        "            duration_ms = cap.get(cv2.CAP_PROP_POS_MSEC) or 0.0\n",
        "            total_frames = max(1, int((duration_ms/1000.0) * fps))\n",
        "\n",
        "        # ---- 等間隔にフレーム番号を抽出 ----\n",
        "        k = min(k, max(1, total_frames))\n",
        "        step = total_frames / float(k)\n",
        "        indices = [int(i * step) for i in range(k)]\n",
        "\n",
        "        results = []\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ok, frame = cap.read()\n",
        "            if not ok or frame is None:\n",
        "                continue\n",
        "            ts_sec = idx / max(fps, 1e-6)\n",
        "            out_path = str(Path(out_dir) / f\"{prefix}_{idx:06d}.jpg\")\n",
        "            cv2.imwrite(out_path, frame, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n",
        "            results.append({\"index\": idx, \"timestamp_sec\": ts_sec, \"path\": out_path})\n",
        "\n",
        "        cap.release()\n",
        "        return results\n",
        "\n",
        "    def _fmt_time(self, t: float) -> str:\n",
        "        \"\"\"秒(float) -> ffmpeg向けの 'HH:MM:SS.mmm' 文字列\"\"\"\n",
        "        if t < 0:\n",
        "            t = 0.0\n",
        "        h = int(t // 3600)\n",
        "        m = int((t % 3600) // 60)\n",
        "        s = t - h*3600 - m*60\n",
        "        return f\"{h:02d}:{m:02d}:{s:06.3f}\"\n",
        "\n",
        "    def _sanitize(self, s: str) -> str:\n",
        "        \"\"\"ファイル名に使いやすいようにサニタイズ\"\"\"\n",
        "        s = s.strip()\n",
        "        s = re.sub(r\"\\s+\", \"_\", s)\n",
        "        s = re.sub(r\"[^A-Za-z0-9._-]\", \"_\", s)\n",
        "        return s[:80] if len(s) > 80 else s\n",
        "\n",
        "\n",
        "    def cut_clip(\n",
        "        self,\n",
        "        id: int,\n",
        "        video_name: str,\n",
        "        video_path: str,\n",
        "        start_time: float,\n",
        "        end_time: float,\n",
        "        out_dir: str = \"clips\",\n",
        "        accurate: bool = True\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        自分の start_time/end_time を使って video_path からクリップを作成\n",
        "        accurate=True: 再エンコードしてフレーム精度で切る\n",
        "        accurate=False: 高速コピーだがキーフレーム単位になる\n",
        "        戻り値: 出力ファイルパス\n",
        "        \"\"\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        ss = self._fmt_time(start_time)\n",
        "        to = self._fmt_time(end_time)\n",
        "\n",
        "        # 出力ファイル名\n",
        "        base = f\"{id:04d}_{ss.replace(':','-')}_{to.replace(':','-')}.mp4\"\n",
        "        out_path = os.path.join(out_dir, base)\n",
        "\n",
        "        if accurate:\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "                \"-i\", video_path, \"-ss\", ss, \"-to\", to,\n",
        "                \"-c:v\", \"libx264\", \"-c:a\", \"aac\", \"-preset\", \"medium\", \"-crf\", \"23\",\n",
        "                \"-movflags\", \"+faststart\",\n",
        "                out_path\n",
        "            ]\n",
        "        else:\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "                \"-ss\", ss, \"-to\", to, \"-i\", video_path, \"-c\", \"copy\",\n",
        "                out_path\n",
        "            ]\n",
        "\n",
        "        subprocess.run(cmd, check=True)\n",
        "        return out_path\n",
        "\n",
        "    def chat(self, image_paths: Dict[str, Any], prompt: str = \"これらの画像から，接客対話に向けてお客さんに発話するべき内容をいくつか生成してください．\") -> None:\n",
        "        \"\"\"\n",
        "        複数画像を含んだ会話サンプル\n",
        "        \"\"\"\n",
        "\n",
        "        # プロンプトの初期化\n",
        "        self.set_inital_prompt()\n",
        "\n",
        "        # まずユーザ発話（テキスト部分）\n",
        "        content = [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": prompt\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # 画像をすべて追加\n",
        "        for path in image_paths:\n",
        "            image_url = self.encode_image(image_path=path[\"path\"])\n",
        "            content.append(\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": image_url}\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # プロンプトに追加\n",
        "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "        # GPT の推論\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.gpt_version,\n",
        "            messages=self.messages\n",
        "        )\n",
        "\n",
        "        # 結果の表示\n",
        "        answer = response.choices[0].message.content\n",
        "        print(answer)\n",
        "        return answer\n",
        "\n",
        "    def encode_image(self, image_path: str) -> str:\n",
        "        \"\"\"chatgptが必要とする形式にエンコードする関数\n",
        "        \"\"\"\n",
        "        _, image_extension = os.path.splitext(image_path)\n",
        "\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "            url = f\"data:image/{image_extension};base64,{image_base64}\"\n",
        "\n",
        "        return url\n",
        "\n",
        "    def run_and_save_nonverbal_outputs(self, frames, data_id, target_video_path: str, csv_path=\"nonverbal_outputs.csv\"):\n",
        "        \"\"\"\n",
        "        cls.satsudora_dataset を走査し、各 data に対する3種類の出力を CSV に追記保存する。\n",
        "        - columns: dataset_idx, data_idx, objective_nonverbal, subjective_nonverbal, answer_obj, answer_sub, answer_both\n",
        "        \"\"\"\n",
        "        out_path = Path(csv_path)\n",
        "        need_header = not out_path.exists()\n",
        "\n",
        "        # 文字化けしにくい utf-8-sig、改行は newline=\"\" を推奨\n",
        "        with out_path.open(\"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            if need_header:\n",
        "                writer.writerow([\n",
        "                    # \"dataset_idx\",\n",
        "                    \"dataset_path\",\n",
        "                    \"data_idx\",\n",
        "                    # \"objective_nonverbal\",\n",
        "                    # \"subjective_nonverbal\",\n",
        "                    \"answer_obj\",\n",
        "                    # \"answer_sub\",\n",
        "                    # \"answer_both\",\n",
        "                ])\n",
        "\n",
        "            # for ds_idx, dataset in tqdm(enumerate(cls.satsudora_dataset)):\n",
        "            #     # print(dataset.data)  # 必要ならログを残す\n",
        "            #     target_video_path = dataset.video_path\n",
        "            #     for data_idx, data in enumerate(dataset.data):\n",
        "            # obj = getattr(data, \"objective_nonverbal\", \"\") or \"\"\n",
        "            # sub = getattr(data, \"subjective_nonverbal\", \"\") or \"\"\n",
        "\n",
        "            # 空文字チェック（どちらかが非空なら実行）\n",
        "            # if obj != \"\" or sub != \"\":\n",
        "            try:\n",
        "                answer_obj = self.chat(\n",
        "                    frames,\n",
        "                    prompt=(\n",
        "                        \"動画内に映っている顧客の非言語的行動を出力してください\"\n",
        "                        # f\"客観的非言語行動は {obj} です．出力は発話文，その理由という形式で出してください．\"\n",
        "                    ),\n",
        "                )\n",
        "            except Exception as e:\n",
        "                answer_obj = f\"[ERROR] {e}\"\n",
        "\n",
        "                # try:\n",
        "                #     answer_sub = cls.chat(\n",
        "                #         frames,\n",
        "                #         prompt=(\n",
        "                #             \"動画内に映っている顧客の非言語的行動に着目しながら，発話文を作成してください．\"\n",
        "                #             f\"主観的な非言語的行動は {sub} です．出力は発話文，その理由という形式で出してください．\"\n",
        "                #         ),\n",
        "                #     )\n",
        "                # except Exception as e:\n",
        "                #     answer_sub = f\"[ERROR] {e}\"\n",
        "\n",
        "                # try:\n",
        "                #     answer_both = cls.chat(\n",
        "                #         frames,\n",
        "                #         prompt=(\n",
        "                #             \"動画内に映っている顧客の非言語的行動に着目しながら，発話文を作成してください．\"\n",
        "                #             f\"客観的非言語行動は {obj} です．主観的な非言語的行動は {sub} です．\"\n",
        "                #             \"出力は発話文，その理由という形式で出してください．\"\n",
        "                #         ),\n",
        "                #     )\n",
        "                # except Exception as e:\n",
        "                #     answer_both = f\"[ERROR] {e}\"\n",
        "\n",
        "                # 行を追記\n",
        "                # print(f\"save {target_video_path}'s data_idx {data_idx}\")\n",
        "            writer.writerow([\n",
        "                # ds_idx,\n",
        "                target_video_path,\n",
        "                data_id,\n",
        "                # obj,\n",
        "                # sub,\n",
        "                answer_obj,\n",
        "                # \" \",\n",
        "                # \" \",\n",
        "                # answer_sub,\n",
        "                # answer_both,\n",
        "            ])\n",
        "            print(answer_obj)\n"
      ],
      "metadata": {
        "id": "Jxgxluo4SD2w"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_stats(values: List[float]):\n",
        "    if not values:\n",
        "        raise ValueError(\"空のリストです。値を1つ以上与えてください。\")\n",
        "\n",
        "    n = len(values)\n",
        "    mean = sum(values) / n\n",
        "\n",
        "    # 不偏分散ではなく母分散（nで割る）\n",
        "    variance = sum((x - mean) ** 2 for x in values) / n\n",
        "    std_dev = math.sqrt(variance)\n",
        "\n",
        "    return mean, variance, std_dev\n",
        "\n",
        "\n",
        "def get_video_duration(video_path: str) -> float:\n",
        "    \"\"\"\n",
        "    動画ファイルの長さを秒数で返す関数\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"動画を開けませんでした: {video_path}\")\n",
        "\n",
        "    # 総フレーム数とFPSを取得\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if fps <= 0:\n",
        "        raise RuntimeError(\"FPS情報を取得できませんでした。\")\n",
        "\n",
        "    duration = frame_count / fps\n",
        "    return duration\n"
      ],
      "metadata": {
        "id": "eaZ6FSbiSAWu"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls = LoadDataset()\n",
        "datasets = cls.load_satsudora_datasets(\"/content/drive/MyDrive/ResearchIntern/EscortData/202501_サツドラ/【最終版】データセット/annotations_csv\")\n",
        "\n",
        "work_dir = \"/content/drive/MyDrive/ResearchIntern/InteractiveVQA/temp_data\"\n",
        "work_video_dir = \"/content/drive/MyDrive/ResearchIntern/InteractiveVQA/temp_video_v3\"\n",
        "\n",
        "start_time = -1\n",
        "end_time = -1\n",
        "\n",
        "json_output_data = []\n",
        "num = 0\n",
        "elapsed_time = 0.\n",
        "elapsed_times = []\n",
        "window_size = 2\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "    target_video_path = dataset.video_path\n",
        "    segments = []\n",
        "    objective_infos = []\n",
        "    video_duration = get_video_duration(dataset.video_path)\n",
        "    filename = os.path.basename(target_video_path).replace(\" のコピー\", \"\")\n",
        "    for id, start in enumerate(range(0, int(video_duration), window_size)):\n",
        "        clip_video_path = cls.cut_clip(id=id, video_name=filename, video_path=target_video_path, start_time=start, end_time=(start+window_size), out_dir=work_video_dir)\n",
        "        frames = cls.extract_uniform_frames(clip_video_path, out_dir=work_dir, rewrite=True)\n",
        "        cls.run_and_save_nonverbal_outputs(frames, id, target_video_path, csv_path=\"/content/drive/MyDrive/ResearchIntern/nonverbal_outputs_v3.csv\")\n",
        "    break\n",
        "\n",
        "\n",
        "#     for idx, data in enumerate(dataset.data):\n",
        "#         if data.objective_nonverbal != \"\" or data.subjective_nonverbal != \"\":\n",
        "#             # ディレクトリ部分を除去\n",
        "#             clip_video_path = cls.cut_clip(id=idx, video_path=target_video_path, start_time=0.0, end_time=data.action_end_time, out_dir=work_video_dir)\n",
        "#             # filename = os.path.basename(target_video_path).replace(\" のコピー\", \"\")\n",
        "#             if start_time == data.action_start_time and end_time == data.action_end_time:\n",
        "#                 continue\n",
        "\n",
        "#             start_time = data.action_start_time\n",
        "#             end_time = data.action_end_time\n",
        "#             elapsed_time += end_time - start_time\n",
        "#             elapsed_times.append((end_time - start_time))\n",
        "#             segments.append([start_time, end_time])\n",
        "#             objective_infos.append(data.objective_nonverbal)\n",
        "\n",
        "#     num += len(objective_infos)\n",
        "\n",
        "#     json_output_data.append({\n",
        "#             \"video_path\": target_video_path,\n",
        "#             \"positive_segments\": segments,\n",
        "#             \"objective\": objective_infos\n",
        "#         }\n",
        "#     )\n",
        "\n",
        "# out_path = \"/content/drive/MyDrive/ResearchIntern/InteractiveVQA/annotation/action_segments_temp.json\"\n",
        "# with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(json_output_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# print(f\"アノテーション数{num}\")\n",
        "# print(f\"総アノテーション時間 {elapsed_time}\")\n",
        "# mean, var, std = calc_stats(elapsed_times)\n",
        "# print(elapsed_times)\n",
        "# print(f\"平均: {mean}, 分散: {var}, 標準偏差: {std}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWRMFRzOSeJB",
        "outputId": "e03da795-d8cf-4b15-cb04-d123f052725d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **観察された客観的な行動** – お客さんはロボットに接近し、ロボットの方向に軽く手を伸ばしている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – お客さんはロボットに興味を持っている可能性が高い。また、ロボットに質問したり触れたいと考えているかもしれない。\n",
            "\n",
            "3. **根拠** – お客さんの体の向きや手の動きがロボットに向かっている点。接近している動作から興味や関心を抱いていると考えられる。\n",
            "1. **観察された客観的な行動** – お客さんはロボットに接近し、ロボットの方向に軽く手を伸ばしている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – お客さんはロボットに興味を持っている可能性が高い。また、ロボットに質問したり触れたいと考えているかもしれない。\n",
            "\n",
            "3. **根拠** – お客さんの体の向きや手の動きがロボットに向かっている点。接近している動作から興味や関心を抱いていると考えられる。\n",
            "1. **観察された客観的な行動** – お客さんは左手を上げて通り過ぎようとしている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – ロボットを避けて移動しようとしている可能性がある。\n",
            "\n",
            "3. **根拠** – お客さんの左手が上がっており、体が前に進む動きを示していること。ロボットに対して接触を避けるための行動にも見える。\n",
            "1. **観察された客観的な行動** – お客さんは左手を上げて通り過ぎようとしている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – ロボットを避けて移動しようとしている可能性がある。\n",
            "\n",
            "3. **根拠** – お客さんの左手が上がっており、体が前に進む動きを示していること。ロボットに対して接触を避けるための行動にも見える。\n",
            "1. **観察された客観的な行動**  \n",
            "   - お客さんは身体をロボットの方へ向けており、時々頭を少し下げています。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価**  \n",
            "   - お客さんはロボットに興味を持って話を聞いている状態である可能性があります。\n",
            "\n",
            "3. **根拠**  \n",
            "   - お客さんの体の向きがロボットに対して正面を向いていることと、頭を少し傾けている様子から、ロボットとの対話に集中していることが伺えます。\n",
            "1. **観察された客観的な行動**  \n",
            "   - お客さんは身体をロボットの方へ向けており、時々頭を少し下げています。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価**  \n",
            "   - お客さんはロボットに興味を持って話を聞いている状態である可能性があります。\n",
            "\n",
            "3. **根拠**  \n",
            "   - お客さんの体の向きがロボットに対して正面を向いていることと、頭を少し傾けている様子から、ロボットとの対話に集中していることが伺えます。\n",
            "1. **観察された客観的な行動**\n",
            "   - お客さんは右手を上げてカメラに向けて見せている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価**\n",
            "   - 挨拶をしているか、何らかの指示や反応を示している可能性がある。\n",
            "\n",
            "3. **根拠**\n",
            "   - 右手が顔の前で上がっている位置から、意図的なジェスチャーと解釈できる。\n",
            "1. **観察された客観的な行動**\n",
            "   - お客さんは右手を上げてカメラに向けて見せている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価**\n",
            "   - 挨拶をしているか、何らかの指示や反応を示している可能性がある。\n",
            "\n",
            "3. **根拠**\n",
            "   - 右手が顔の前で上がっている位置から、意図的なジェスチャーと解釈できる。\n",
            "1. **観察された客観的な行動** – お客さんは身を屈め、手元に視線を落としている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – お客さんは何かを確認したり操作したりしている可能性がある。\n",
            "\n",
            "3. **根拠** – お客さんの頭の位置が下がっており、目線が下を向いているため。\n",
            "1. **観察された客観的な行動** – お客さんは身を屈め、手元に視線を落としている。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – お客さんは何かを確認したり操作したりしている可能性がある。\n",
            "\n",
            "3. **根拠** – お客さんの頭の位置が下がっており、目線が下を向いているため。\n",
            "1. **観察された客観的な行動** – お客さんは両手をすり合わせながら、ロボットの方に身を乗り出している。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – 寒さから手を温めている、または話しかける準備として緊張を和らげようとしているのかもしれない。\n",
            "\n",
            "3. **根拠** – 両手をすり合わせている行動は、寒さや緊張を和らげる行動として一般的であり、体をロボットの方に向けていることから関心を示していると解釈できる。\n",
            "1. **観察された客観的な行動** – お客さんは両手をすり合わせながら、ロボットの方に身を乗り出している。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価** – 寒さから手を温めている、または話しかける準備として緊張を和らげようとしているのかもしれない。\n",
            "\n",
            "3. **根拠** – 両手をすり合わせている行動は、寒さや緊張を和らげる行動として一般的であり、体をロボットの方に向けていることから関心を示していると解釈できる。\n",
            "I'm sorry, I can't assist with that.\n",
            "I'm sorry, I can't assist with that.\n",
            "1. **観察された客観的な行動**: お客さんはロボットに近づき、手を胸の前で組み、ロボットに身を乗り出している。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価**: ロボットに興味を持っている、またはロボットとのインタラクションを楽しんでいる可能性がある。\n",
            "\n",
            "3. **根拠**: 身体の傾きと手の位置から、ロボットに注意を向けている。また、ロボットに近づいていることが、その行動を示唆している。\n",
            "1. **観察された客観的な行動**: お客さんはロボットに近づき、手を胸の前で組み、ロボットに身を乗り出している。\n",
            "\n",
            "2. **観察された行動に対する主観的な評価**: ロボットに興味を持っている、またはロボットとのインタラクションを楽しんでいる可能性がある。\n",
            "\n",
            "3. **根拠**: 身体の傾きと手の位置から、ロボットに注意を向けている。また、ロボットに近づいていることが、その行動を示唆している。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIyL4ytnWY1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG, POS = 0, 1\n",
        "\n",
        "def _merge_intervals(intervals: List[Tuple[float,float]]) -> List[Tuple[float,float]]:\n",
        "    xs = []\n",
        "    for s, e in intervals:\n",
        "        if s is None or e is None:\n",
        "            continue\n",
        "        s, e = float(s), float(e)\n",
        "        if e <= s:\n",
        "            continue\n",
        "        xs.append((min(s, e), max(s, e)))\n",
        "    if not xs:\n",
        "        return []\n",
        "    xs.sort(key=lambda x: x[0])\n",
        "    out = [list(xs[0])]\n",
        "    for s, e in xs[1:]:\n",
        "        if s <= out[-1][1]:\n",
        "            out[-1][1] = max(out[-1][1], e)\n",
        "        else:\n",
        "            out.append([s, e])\n",
        "    return [tuple(x) for x in out]\n",
        "\n",
        "def _in_any_interval(t: float, intervals: List[Tuple[float,float]]) -> bool:\n",
        "    for s, e in intervals:\n",
        "        if s <= t < e:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def build_positive_intervals(entry: Dict[str, Any], pos_window: float) -> List[Tuple[float, float]]:\n",
        "    out = []\n",
        "    for s, e in entry.get(\"positive_segments\", []) or []:\n",
        "        out.append((float(s), float(e)))\n",
        "    half = pos_window / 2.0\n",
        "    for t in entry.get(\"positives\", []) or []:\n",
        "        t = float(t)\n",
        "        out.append((t - half, t + half))\n",
        "    out = [(min(s, e), max(s, e)) for (s, e) in out if e != s]\n",
        "    return _merge_intervals(out)\n",
        "\n",
        "def in_any_interval(t: float, intervals: List[Tuple[float, float]]) -> bool:\n",
        "    for s, e in intervals:\n",
        "        if s <= t < e:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    \"\"\"Frozen ResNet18 -> 512-d feature\"\"\"\n",
        "    def __init__(self, resize=224, center_crop=224):\n",
        "        super().__init__()\n",
        "        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # [B,512,1,1]\n",
        "        for p in self.backbone.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.tf = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.CenterCrop(center_crop),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "        ])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        f = self.backbone(x).view(x.size(0), -1)\n",
        "        return f\n",
        "\n",
        "class AudioCNN(nn.Module):\n",
        "    \"\"\"Log-mel (B,1,n_mels,Tm) -> 128-d\"\"\"\n",
        "    def __init__(self, n_mels=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.proj = nn.Linear(64, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x).flatten(1)\n",
        "        z = self.proj(h)\n",
        "        return z\n",
        "\n",
        "class AV_LSTM(nn.Module):\n",
        "    \"\"\"BiLSTM over [vis512 + aud128] -> logits(2)\"\"\"\n",
        "    def __init__(self, in_dim=512+128, hidden_dim=256, num_layers=1, bidir=True, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=in_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
        "            dropout=0.0 if num_layers == 1 else dropout, bidirectional=bidir, batch_first=True\n",
        "        )\n",
        "        out_dim = hidden_dim * (2 if bidir else 1)\n",
        "        self.cls = nn.Linear(out_dim, 2)\n",
        "\n",
        "    def forward(self, x_packed):\n",
        "        y_packed, _ = self.lstm(x_packed)\n",
        "        y, _ = pad_packed_sequence(y_packed, batch_first=True)\n",
        "        logits = self.cls(y)\n",
        "        return logits\n",
        "\n",
        "class AVBinaryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    入力JSON形式:\n",
        "    [\n",
        "      {\"video_path\": \".../video1.mp4\", \"positive_segments\": [[s,e], ...]},\n",
        "      {\"video_path\": \".../video2.mp4\", \"positive_segments\": [[s,e], ...]},\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dataset_json: str,\n",
        "                 split: str = \"train\",\n",
        "                 split_ratio: float = 0.8,\n",
        "                 seed: int = 42,\n",
        "                 target_fps: float = 2.0,\n",
        "                 audio_sr: int = 16000,\n",
        "                 audio_win: float = 0.5,\n",
        "                 n_mels: int = 64,\n",
        "                 n_fft: int = 400,\n",
        "                 hop_length: int = 160,\n",
        "                 resize: int = 224,\n",
        "                 center_crop: int = 224):\n",
        "        super().__init__()\n",
        "        assert split in (\"train\", \"val\")\n",
        "        self.target_fps = float(target_fps)\n",
        "        self.audio_sr = int(audio_sr)\n",
        "        self.audio_win = float(audio_win)\n",
        "        self.n_mels = int(n_mels)\n",
        "        self.n_fft = int(n_fft)\n",
        "        self.hop_length = int(hop_length)\n",
        "        self.resize = int(resize)\n",
        "        self.center_crop = int(center_crop)\n",
        "\n",
        "        with open(dataset_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            items = json.load(f)\n",
        "        # 事前にマージ・正規化\n",
        "        for it in items:\n",
        "            segs = it.get(\"positive_segments\", []) or []\n",
        "            it[\"positive_segments\"] = _merge_intervals([(float(s), float(e)) for s, e in segs])\n",
        "\n",
        "        # split\n",
        "        rng = np.random.default_rng(seed)\n",
        "        idxs = np.arange(len(items)); rng.shuffle(idxs)\n",
        "        cut = int(len(idxs) * split_ratio)\n",
        "        self.items = [items[i] for i in (idxs[:cut] if split==\"train\" else idxs[cut:])]\n",
        "\n",
        "        self.tf = transforms.Compose([\n",
        "            transforms.Resize(self.resize),\n",
        "            transforms.CenterCrop(self.center_crop),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def _sample_video(self, path: str):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        if not cap.isOpened():\n",
        "            raise RuntimeError(f\"Failed to open video: {path}\")\n",
        "        vfps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        if vfps <= 1e-3: vfps = 30.0\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        duration = total / vfps if total > 0 else None\n",
        "\n",
        "        frames, ts_list = [], []\n",
        "        step = 1.0 / self.target_fps\n",
        "        t = 0.0\n",
        "        while duration is None or t < duration:\n",
        "            idx = int(round(t * vfps))\n",
        "            if total > 0 and idx >= total: break\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ok, f = cap.read()\n",
        "            if not ok: break\n",
        "            frames.append(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))\n",
        "            ts_list.append(t)\n",
        "            t += step\n",
        "        cap.release()\n",
        "        if duration is None:\n",
        "            duration = len(ts_list) * step\n",
        "        return frames, ts_list, duration\n",
        "\n",
        "    def _read_audio(self, path: str):\n",
        "        y, sr = librosa.load(path, sr=self.audio_sr, mono=True)\n",
        "        return y, sr\n",
        "\n",
        "    def _logmel(self, y: np.ndarray, sr: int) -> np.ndarray:\n",
        "        S = librosa.feature.melspectrogram(\n",
        "            y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length,\n",
        "            n_mels=self.n_mels, power=2.0\n",
        "        )\n",
        "        return librosa.power_to_db(S, ref=np.max)  # [n_mels, Tm]\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        it   = self.items[idx]\n",
        "        path = it[\"video_path\"]\n",
        "        pos_intervals = it.get(\"positive_segments\", [])\n",
        "\n",
        "        frames, ts_list, vdur = self._sample_video(path)\n",
        "        audio, sr = self._read_audio(path)\n",
        "        adur = len(audio) / sr\n",
        "        total_dur = min(vdur, adur)\n",
        "\n",
        "        # labels\n",
        "        labels = [POS if _in_any_interval(t, pos_intervals) else NEG for t in ts_list]\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        # visual tensors\n",
        "        v_tensors = [self.tf(transforms.functional.to_pil_image(img)) for img in frames]\n",
        "        if len(v_tensors) == 0:\n",
        "            v_tensors = [torch.zeros(3, self.center_crop, self.center_crop)]\n",
        "            labels = torch.tensor([NEG], dtype=torch.long)\n",
        "            ts_list = [0.0]\n",
        "        frames_tensor = torch.stack(v_tensors, 0)  # [T,3,H,W]\n",
        "\n",
        "        # audio windows -> log-mel\n",
        "        half = self.audio_win / 2.0\n",
        "        specs = []\n",
        "        for t in ts_list:\n",
        "            a0 = max(0.0, t - half)\n",
        "            a1 = min(total_dur, t + half)\n",
        "            s0 = int(round(a0 * sr)); s1 = int(round(a1 * sr))\n",
        "            if s1 <= s0: s1 = min(len(audio), s0 + self.hop_length)\n",
        "            y = audio[s0:s1]\n",
        "            if len(y) < self.n_fft:\n",
        "                pad = self.n_fft - len(y)\n",
        "                y = np.pad(y, (0, pad), mode='reflect')\n",
        "            m = self._logmel(y, sr)  # [n_mels, Tm]\n",
        "            specs.append(torch.from_numpy(m).float().unsqueeze(0))  # [1,n_mels,Tm]\n",
        "\n",
        "        return {\n",
        "            \"frames\": frames_tensor,   # [T,3,H,W]\n",
        "            \"specs\": specs,            # list of [1,n_mels,Tm]\n",
        "            \"labels\": labels,          # [T]\n",
        "            \"length\": len(labels),\n",
        "            \"video_path\": path,\n",
        "        }"
      ],
      "metadata": {
        "id": "uWULYUpMRMiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_var(batch):\n",
        "    lengths = torch.tensor([b[\"length\"] for b in batch], dtype=torch.long)\n",
        "    max_len = int(lengths.max().item())\n",
        "    labels = torch.full((len(batch), max_len), -100, dtype=torch.long)\n",
        "    for i, b in enumerate(batch):\n",
        "        labels[i, :b[\"length\"]] = b[\"labels\"]\n",
        "    return {\n",
        "        \"frames_list\": [b[\"frames\"] for b in batch],\n",
        "        \"specs_list\":  [b[\"specs\"]  for b in batch],\n",
        "        \"labels\": labels,\n",
        "        \"lengths\": lengths,\n",
        "        \"video_paths\": [b[\"video_path\"] for b in batch],\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_visual_features(frames_seq: torch.Tensor, venc: CNNEncoder, chunk=64) -> torch.Tensor:\n",
        "    T = frames_seq.shape[0]\n",
        "    out = []\n",
        "    for i in range(0, T, chunk):\n",
        "        f = venc(frames_seq[i:i+chunk].to(device, non_blocking=True))  # [B,512]\n",
        "        out.append(f.cpu())\n",
        "    return torch.cat(out, 0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_audio_features(specs_seq: List[torch.Tensor], aenc: AudioCNN, chunk=64) -> torch.Tensor:\n",
        "    out, buf = [], []\n",
        "    for m in specs_seq:\n",
        "        buf.append(m)\n",
        "        if len(buf) == chunk:\n",
        "            tm = max(x.shape[-1] for x in buf)\n",
        "            batch = torch.zeros(len(buf), 1, buf[0].shape[1], tm, dtype=torch.float32)\n",
        "            for i, x in enumerate(buf):\n",
        "                batch[i, :, :, :x.shape[-1]] = x\n",
        "            out.append(aenc(batch.to(device, non_blocking=True)).cpu())\n",
        "            buf = []\n",
        "    if buf:\n",
        "        tm = max(x.shape[-1] for x in buf)\n",
        "        batch = torch.zeros(len(buf), 1, buf[0].shape[1], tm, dtype=torch.float32)\n",
        "        for i, x in enumerate(buf):\n",
        "            batch[i, :, :, :x.shape[-1]] = x\n",
        "        out.append(aenc(batch.to(device, non_blocking=True)).cpu())\n",
        "    return torch.cat(out, 0)\n",
        "\n",
        "def train_one_epoch(model, venc, aenc, loader, opt, pos_weight: Optional[float]):\n",
        "    model.train()\n",
        "    if pos_weight is not None:\n",
        "        ce = nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor([1.0, pos_weight], device=device))\n",
        "    else:\n",
        "        ce = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    total_loss = total_tok = total_cor = 0\n",
        "    for batch in loader:\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        B = labels.size(0)\n",
        "        lengths = batch[\"lengths\"].clone()\n",
        "        max_len = int(lengths.max().item())\n",
        "\n",
        "        feats_pad = torch.zeros(B, max_len, 512+128, dtype=torch.float32)\n",
        "        for i in range(B):\n",
        "            vseq = extract_visual_features(batch[\"frames_list\"][i], venc)   # [T,512]\n",
        "            aseq = extract_audio_features(batch[\"specs_list\"][i], aenc)     # [T,128]\n",
        "            T = min(vseq.size(0), aseq.size(0))\n",
        "            feats_pad[i, :T] = torch.cat([vseq[:T], aseq[:T]], dim=1)\n",
        "            lengths[i] = T\n",
        "        feats_pad = feats_pad.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        packed = pack_padded_sequence(feats_pad, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        logits = model(packed)  # [B,T,2]\n",
        "\n",
        "        loss = ce(logits.reshape(-1, 2), labels.reshape(-1))\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask = labels.ne(-100)\n",
        "            pred = logits.argmax(-1)\n",
        "            cor = (pred.eq(labels) & mask).sum().item()\n",
        "            total_cor += cor\n",
        "            total_tok += mask.sum().item()\n",
        "            total_loss += loss.item() * mask.sum().item()\n",
        "\n",
        "    return total_loss / max(total_tok, 1), total_cor / max(total_tok, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, venc, aenc, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    total_loss = total_tok = total_cor = 0\n",
        "    for batch in loader:\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        B = labels.size(0)\n",
        "        lengths = batch[\"lengths\"].clone()\n",
        "        max_len = int(lengths.max().item())\n",
        "\n",
        "        feats_pad = torch.zeros(B, max_len, 512+128, dtype=torch.float32)\n",
        "        for i in range(B):\n",
        "            vseq = extract_visual_features(batch[\"frames_list\"][i], venc)\n",
        "            aseq = extract_audio_features(batch[\"specs_list\"][i], aenc)\n",
        "            T = min(vseq.size(0), aseq.size(0))\n",
        "            feats_pad[i, :T] = torch.cat([vseq[:T], aseq[:T]], dim=1)\n",
        "            lengths[i] = T\n",
        "        feats_pad = feats_pad.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        packed = pack_padded_sequence(feats_pad, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        logits = model(packed)\n",
        "\n",
        "        loss = ce(logits.reshape(-1, 2), labels.reshape(-1))\n",
        "        mask = labels.ne(-100)\n",
        "        pred = logits.argmax(-1)\n",
        "        cor = (pred.eq(labels) & mask).sum().item()\n",
        "        total_cor += cor\n",
        "        total_tok += mask.sum().item()\n",
        "        total_loss += loss.item() * mask.sum().item()\n",
        "\n",
        "    return total_loss / max(total_tok, 1), total_cor / max(total_tok, 1)\n"
      ],
      "metadata": {
        "id": "Tm2ltkejRSLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_video_binary(video_path: str, model: AV_LSTM, venc: CNNEncoder, aenc: AudioCNN, cfg=CFG):\n",
        "    # 単体動画のフレーム列を構築（NEG/ POS ラベルは作らない）\n",
        "    tmp_ann = [{\"video_path\": video_path}]\n",
        "    tmp_ts  = [{\"video_path\": video_path, \"positive_segments\": []}]  # ラベル不要のため空\n",
        "    ann_p = \"/content/_tmp_ann.json\"\n",
        "    ts_p  = \"/content/_tmp_ts.json\"\n",
        "    json.dump(tmp_ann, open(ann_p, \"w\")); json.dump(tmp_ts, open(ts_p, \"w\"))\n",
        "    tmp_cfg = dict(cfg); tmp_cfg[\"annotations\"] = ann_p; tmp_cfg[\"binary_timestamps\"] = ts_p\n",
        "\n",
        "    ds = AVBinaryDataset(tmp_cfg, split=\"train\")  # 中でサンプリングや音声切り出しだけ使う\n",
        "    rec = ds[0]\n",
        "    frames_seq = rec[\"frames\"]\n",
        "    specs_seq  = rec[\"specs\"]\n",
        "\n",
        "    v = extract_visual_features(frames_seq, venc)       # [T,512]\n",
        "    a = extract_audio_features(specs_seq, aenc)         # [T,128]\n",
        "    T = min(v.size(0), a.size(0))\n",
        "    feats = torch.cat([v[:T], a[:T]], dim=1).unsqueeze(0).to(device)  # [1,T,640]\n",
        "    lengths = torch.tensor([T])\n",
        "    packed = pack_padded_sequence(feats, lengths, batch_first=True, enforce_sorted=False)\n",
        "    logits = model(packed)                              # [1,T,2]\n",
        "    probs = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()    # [T,2]\n",
        "    preds = probs.argmax(-1)                            # [T]\n",
        "    return preds, probs\n"
      ],
      "metadata": {
        "id": "moHyPMF4RYyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fq6psP3IaQ8r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}